{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 – Importa le librerie e carica il dataset\n",
    "\n",
    "Come prima cosa importiamo le librerie e tutti i moduli che utilizzeremo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Innanzitutto creiamo una cella per le opzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "PATH = \"./dataset\"\n",
    "\n",
    "# ratios+\n",
    "TEST_RATIO = 0.2\n",
    "VALIDATION_RATIO = 0.2\n",
    "IMG_DIMS = (32, 32)\n",
    "\n",
    "# model\n",
    "FILTERS = 60\n",
    "FILTER1 = (5, 5)\n",
    "FILTER2 = (3, 3)\n",
    "POOL_SIZE = (2, 2)\n",
    "NODES = 500\n",
    "\n",
    "# training\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 10\n",
    "STEPS = 131\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importiamo il dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list directories inside PATH\n",
    "__dir_list = os.listdir(PATH)\n",
    "print(f\"Total num of classes detected: {len(__dir_list)}\")  # should be 10\n",
    "\n",
    "# store the number of folders. the latter is also the number of classes\n",
    "__classes = len(__dir_list)\n",
    "\n",
    "# import all images and insert them inside a single list\n",
    "print(\"Importing classes...\")\n",
    "\n",
    "images, class_ids = [], []\n",
    "# loop folders to obtain images inside them\n",
    "for _class in range(__classes):\n",
    "    __img_list = os.listdir(f\"{PATH}/{str(_class)}\")\n",
    "\n",
    "    # for every folder loop through the images\n",
    "    for _img in __img_list:\n",
    "        current_img = cv2.imread(f\"{PATH}/{str(_class)}/{str(_img)}\")\n",
    "\n",
    "        # images are too big to be processed efficiently by the network thus\n",
    "        # a resize is needed (180×180 -> 32×32)\n",
    "        current_img = cv2.resize(current_img, IMG_DIMS)\n",
    "        images.append(current_img)\n",
    "\n",
    "        # save the corresponding ID (class ID) of each image\n",
    "        class_ids.append(_class)\n",
    "    print(_class, end=\" \")\n",
    "print(\"\\nDone\")  # reset previous end command\n",
    "\n",
    "\n",
    "# convert lists to numpy arrays\n",
    "images = np.array(images)\n",
    "class_ids = np.array(class_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 – Divisione dei dati\n",
    "\n",
    "Bisogna dividere il dataset in $3$ subsets: _training_, _testing_ e _validation_.\n",
    "Utilizziamo la funzione `train_test_split()` di `sklearn.model_selection`.\n",
    "\n",
    "Dividere i dati è necessario perché permette di mescolare le classi. Se così\n",
    "non fosse, ad esempio, il l'80% dei dati conterrebbe le immagini fino alla\n",
    "cartella $7$ e non includerebbe $8$ e $9$. Per questo è necessario un pacchetto\n",
    "che mischi i dati e li divida equamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training = 0.8, testing = 0.2\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        images, class_ids, test_size=TEST_RATIO\n",
    "    )\n",
    "\n",
    "# training = 0.8, validation = 0.2\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(\n",
    "    X_train, Y_train, test_size=VALIDATION_RATIO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Piccolo controllo delle `shape`s dei set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train set:      {X_train.shape}\")\n",
    "print(f\"Test set:       {X_test.shape}\")\n",
    "print(f\"Validation set: {X_validation.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controlliamo che la distribuzione dei dati sia uniforme perché non vogliamo\n",
    "favorire nessuna classe. Recuperiamo queste informazioni dal `Y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_samples = [len(np.where(Y_train == i)[0]) for i in range(__classes)]\n",
    "print(_samples)\n",
    "\n",
    "# visualize the data with histogram\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(__classes), _samples)\n",
    "plt.title(\"Number of images for each class\")\n",
    "plt.xlabel(\"Class ID\")\n",
    "plt.ylabel(\"Number of images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 – Pre-processare i dati\n",
    "\n",
    "Le immagini non possono essere consegnate direttamente al modello, sarà necessario\n",
    "processare i dati per prepararli per la rete neurale. Dopo il pre-processing \n",
    "la `X_train.shape` passa da `(6502, 32, 32, 3)` a `(6502, 32, 32)`. La rete\n",
    "neurale necessita un'altra dimensione, dunque `(6502, 32, 32, 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # grayscale\n",
    "    image = cv2.equalizeHist(image)  # equalize img (distribute lighting evenly)\n",
    "    image = image / 255  # normalize img (restrict from [0, 255] to [0, 1])\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "# preprocess all the images using map() &overwrite arrays\n",
    "X_train = np.array(list(map(preprocess, X_train)))\n",
    "X_test = np.array(list(map(preprocess, X_test)))\n",
    "X_validation = np.array(list(map(preprocess, X_validation)))\n",
    "\n",
    "# add depth to images\n",
    "X_train = X_train.reshape(X_train.shape + (1,))\n",
    "X_test = X_test.reshape(X_test.shape + (1,))\n",
    "X_validation = X_validation.reshape(X_validation.shape + (1,))\n",
    "\n",
    "print(f\"X_train.shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora è necessario modificare le immagini rendendole mosse, spostandole, zoomando\n",
    "per rendere il dataset più generico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataGenerator = ImageDataGenerator(\n",
    "    width_shift_range  = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    zoom_range         = 0.2,\n",
    "    shear_range        = 0.1,\n",
    "    rotation_range     = 0.1,\n",
    ")\n",
    "\n",
    "DataGenerator.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successivamente effettuiamo il One-Hot encoding delle matrici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = to_categorical(Y_train, __classes)\n",
    "Y_test = to_categorical(Y_test, __classes)\n",
    "Y_validation = to_categorical(Y_validation, __classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 – Creare il modello\n",
    "\n",
    "Creiamo ora il modello per la CNN. Questa generalmente consiste di convolutional\n",
    "e pooling layers. Funziona meglio per dati rappresentati da strutture a griglia, \n",
    "per questo funziona bene per la classificazione delle immagini.\n",
    "\n",
    "Il layer di dropout è utilizzato per disattivare alcuni neuroni riducendo\n",
    "l'over fitting del modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(\n",
    "    filters_num: int,\n",
    "    filter1: tuple, filter2: tuple,\n",
    "    input_shape: tuple,\n",
    "    pool_size: tuple,\n",
    "    nodes_num: int):\n",
    "\n",
    "    # generate model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add convolutional layers (×2)\n",
    "    model.add(\n",
    "        Conv2D(\n",
    "            filters_num,\n",
    "            filter1,\n",
    "            input_shape=(input_shape + (1,)),\n",
    "            activation = \"relu\"\n",
    "        )\n",
    "    )\n",
    "    model.add(Conv2D(filters_num, filter1, activation=\"relu\"))\n",
    "    \n",
    "    # add pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "    \n",
    "    # add two more convolutional layers with filter2 and less filters\n",
    "    model.add(Conv2D(filters_num // 2, filter2, activation=\"relu\"))\n",
    "    model.add(Conv2D(filters_num // 2, filter2, activation=\"relu\"))\n",
    "    \n",
    "    # add additional pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "    \n",
    "    # add dropout layer at 50%\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # add flatten layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # add dense (×2) + dropout layers\n",
    "    model.add(Dense(nodes_num, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(__classes, activation=\"softmax\"))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(\n",
    "        Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assegnamo il modello e otteniamo il sommario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(FILTERS, FILTER2, FILTER2, IMG_DIMS, POOL_SIZE, NODES)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 – Addestra il modello\n",
    "\n",
    "La funzione `model.fit()` di `keras` inizierà il training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = model.fit(\n",
    "    DataGenerator.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch = STEPS,\n",
    "    epochs          = EPOCHS,\n",
    "    validation_data = (X_validation, Y_validation),\n",
    "    shuffle         = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostriamo i dati ottenuti dal training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.legend([\"training\", \"validation\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.legend([\"training\", \"validation\"])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controlliamo l'accuratezza del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(f\"Score: {score[0]}\\nAccuracy: {score[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salviamo il modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"trained_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pydoku')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21df5813da2c78d79906d3707346daec9326d692096f20b6334f30695c4e5055"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
